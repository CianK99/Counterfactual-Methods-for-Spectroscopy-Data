{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85535ce0",
   "metadata": {},
   "source": [
    "# Preprocessing and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50553cfc",
   "metadata": {},
   "source": [
    "Raman COVID data available at: https://figshare.com/articles/dataset/Data_and_code_on_serum_Raman_spectroscopy_as_an_efficient_primary_screening_of_coronavirus_disease_in_2019_COVID-19_/12159924 \\\n",
    "Raman Bacteria data available at: https://www.dropbox.com/scl/fo/fb29ihfnvishuxlnpgvhg/AJToUtts-vjYdwZGeqK4k-Y?rlkey=r4p070nsuei6qj3pjp13nwf6l&e=1&dl=0 \\\n",
    "DRS Tissue data available at: https://springernature.figshare.com/collections/Extended-wavelength_diffuse_reflectance_spectroscopy_dataset_of_animal_tissues_for_bone-related_biomedical_applications/6894172/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046aa980",
   "metadata": {},
   "source": [
    "# GLACIER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd6a50",
   "metadata": {},
   "source": [
    "# Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849cc803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import json, hashlib, zipfile, shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ramanspy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.io import loadmat\n",
    "\n",
    "NAME = \"RamanCOVID19_ramanspy_preprocessed\"\n",
    "POS_IDS = {\"COVID-19\"}  # Positive class\n",
    "NEG_IDS = {\"Healthy\"}   # Negative class\n",
    "TEST_SIZE = 0.20\n",
    "RANDOM_SEED = 4\n",
    "\n",
    "# Data paths\n",
    "ROOT = Path.home() / \"local-datasets\" / \"covid19\"\n",
    "OUT_CSV = ROOT / \"covid19_serum_raman_preprocessed.csv\"\n",
    "\n",
    "CLASS_FILES = {\n",
    "    \"COVID-19\":   \"raw_COVID.txt\",\n",
    "    \"Healthy\":    \"raw_Helthy.txt\",\n",
    "    \"Suspected\":  \"raw_Suspected.txt\",\n",
    "}\n",
    "\n",
    "# wildboar repository settings\n",
    "REPO_ROOT = Path.home() / \"local-datasets\"\n",
    "HTTP_BASE = \"http://127.0.0.1:8765\"\n",
    "MANIFEST_NAME = \"repo2.json\"\n",
    "REPO_NAME = \"repotwo\"\n",
    "BUNDLE_VERSION = \"1.2\"\n",
    "BUNDLE_TAG = \"default\"\n",
    "\n",
    "ZIP_NAME_NO_EXT = f\"ucr-v{BUNDLE_VERSION}-{BUNDLE_TAG}\"\n",
    "ZIP_NAME = f\"{ZIP_NAME_NO_EXT}.zip\"\n",
    "SHA_NAME = f\"{ZIP_NAME_NO_EXT}.sha1\"\n",
    "\n",
    "\n",
    "def _sha1(path: Path) -> str:\n",
    "    return hashlib.sha1(path.read_bytes()).hexdigest()\n",
    "\n",
    "\n",
    "def read_matrix(p: Path) -> np.ndarray:\n",
    "    \"\"\"Robust whitespace/CSV reader\"\"\"\n",
    "    return pd.read_csv(p, header=None, sep=r\"[\\s,;]+\", engine=\"python\").values\n",
    "\n",
    "\n",
    "def get_wavenumbers_from_mat(mat_path: Path) -> np.ndarray:\n",
    "    \"\"\"Extract wavenumber vector from MAT file\"\"\"\n",
    "    md = loadmat(mat_path)\n",
    "    cands = []\n",
    "    for k, v in md.items():\n",
    "        if not isinstance(v, np.ndarray):\n",
    "            continue\n",
    "        arr = v.squeeze()\n",
    "        if arr.ndim == 1 and 200 < arr.size < 5000:\n",
    "            if np.all((arr >= 350) & (arr <= 4000)):\n",
    "                cands.append(arr.astype(float))\n",
    "    if not cands:\n",
    "        raise RuntimeError(\"No suitable wavenumber vector found in data.mat\")\n",
    "    return max(cands, key=lambda a: a.size)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Loading raw data...\")\n",
    "    \n",
    "    # Get wavenumbers from MAT file\n",
    "    wn = get_wavenumbers_from_mat(ROOT / \"data.mat\")\n",
    "    B = wn.size\n",
    "    print(f\"Found {B} wavenumbers from {wn.min():.1f} to {wn.max():.1f} cm⁻¹\")\n",
    "\n",
    "    # Read class matrices\n",
    "    X_list, y_list = [], []\n",
    "    for label, fname in CLASS_FILES.items():\n",
    "        mat = read_matrix(ROOT / fname)\n",
    "        # fix orientation\n",
    "        if mat.shape[1] != B and mat.shape[0] == B:\n",
    "            mat = mat.T\n",
    "        elif mat.shape[1] != B and mat.shape[0] != B:\n",
    "            raise ValueError(f\"{fname}: shape {mat.shape} doesn't match wavenumber length {B}\")\n",
    "        Xc = mat.astype(np.float32)\n",
    "        X_list.append(Xc)\n",
    "        y_list.append(np.full(Xc.shape[0], label, dtype=object))\n",
    "        print(f\"{label:10s}: {Xc.shape[0]} samples, {Xc.shape[1]} wavenumbers\")\n",
    "\n",
    "    X_all_raw = np.vstack(X_list)\n",
    "    y_all = np.concatenate(y_list)\n",
    "    \n",
    "    raman_spectra = ramanspy.Spectrum(X_all_raw, wn)\n",
    "\n",
    "    print(\"Applying ramanspy preprocessing...\")\n",
    "    \n",
    "    pipeline = ramanspy.preprocessing.Pipeline([\n",
    "        ramanspy.preprocessing.despike.WhitakerHayes(),\n",
    "        ramanspy.preprocessing.baseline.ASLS(),\n",
    "        ramanspy.preprocessing.normalise.MinMax(),\n",
    "    ])\n",
    "    \n",
    "    X_all_preprocessed = pipeline.apply(raman_spectra)\n",
    "\n",
    "    print(\"Subsetting and splitting preprocessed data...\")\n",
    "    mask = np.isin(y_all, list(POS_IDS | NEG_IDS))\n",
    "    X = X_all_preprocessed.spectral_data[mask].astype(np.float32)\n",
    "    y = y_all[mask]\n",
    "    y_bin = np.where(np.isin(y, list(POS_IDS)), 1, 0).astype(np.int64)\n",
    "\n",
    "    # Save full preprocessed CSV\n",
    "    headers = [f\"{int(v)}\" if float(v).is_integer() else f\"{v:g}\" for v in wn]\n",
    "    df = pd.DataFrame(X_all_preprocessed.spectral_data, columns=headers)\n",
    "    df[\"diagnostic\"] = y_all\n",
    "    df.to_csv(OUT_CSV, index=False)\n",
    "    print(\"Wrote preprocessed CSV ->\", OUT_CSV)\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y_bin, test_size=TEST_SIZE, stratify=y_bin, random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    tmp = REPO_ROOT / \"tmp_ucr_build\"\n",
    "    (tmp / \"ucr\").mkdir(parents=True, exist_ok=True)\n",
    "    np.savez_compressed(tmp / \"ucr\" / f\"{NAME}_TRAIN.npz\", x=X_tr, y=y_tr)\n",
    "    np.savez_compressed(tmp / \"ucr\" / f\"{NAME}_TEST.npz\",  x=X_te, y=y_te)\n",
    "\n",
    "    zip_path = REPO_ROOT / ZIP_NAME\n",
    "    if zip_path.exists():\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            z.extractall(tmp)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
    "        for p in sorted((tmp / \"ucr\").glob(\"*\")):\n",
    "            if p.is_file():\n",
    "                z.write(p, arcname=f\"ucr/{p.name}\")\n",
    "    \n",
    "    (REPO_ROOT / SHA_NAME).write_text(_sha1(zip_path))\n",
    "\n",
    "    manifest_path = REPO_ROOT / MANIFEST_NAME\n",
    "    if manifest_path.exists():\n",
    "        manifest = json.loads(manifest_path.read_text())\n",
    "    else:\n",
    "        manifest = {\n",
    "            \"name\": REPO_NAME,\n",
    "            \"version\": \"1.0\",\n",
    "            \"wildboar_requires\": \"1.0\",\n",
    "            \"bundle_url\": f\"{HTTP_BASE}/{{bundle}}-v{{version}}-{{tag}}\",\n",
    "            \"bundles\": []\n",
    "        }\n",
    "    \n",
    "    manifest[\"bundle_url\"] = f\"{HTTP_BASE}/{{bundle}}-v{{version}}-{{tag}}\"\n",
    "    bundles = manifest.setdefault(\"bundles\", [])\n",
    "    ucr = next((b for b in bundles if b.get(\"key\") == \"ucr\" and b.get(\"version\") == BUNDLE_VERSION and b.get(\"tag\", \"default\") == BUNDLE_TAG), None)\n",
    "    \n",
    "    if ucr is None:\n",
    "        ucr = {\"key\": \"ucr\", \"name\": \"ucr\", \"version\": BUNDLE_VERSION, \"tag\": BUNDLE_TAG, \"datasets\": []}\n",
    "        bundles.append(ucr)\n",
    "\n",
    "    def ensure(part: str):\n",
    "        fname = f\"{NAME}_{'TRAIN' if part == 'train' else 'TEST'}.npz\"\n",
    "        if not any(d.get(\"name\") == NAME and d.get(\"part\") == part for d in ucr[\"datasets\"]):\n",
    "            ucr[\"datasets\"].append({\"name\": NAME, \"file\": fname, \"part\": part})\n",
    "    ensure(\"train\"); ensure(\"test\")\n",
    "\n",
    "    manifest_path.write_text(json.dumps(manifest, indent=2))\n",
    "    shutil.rmtree(tmp, ignore_errors=True)\n",
    "    \n",
    "    print(\"\\n Built and added new preprocessed dataset to the repository.\")\n",
    "    print(f\"   Dataset name: '{NAME}'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c63c0f",
   "metadata": {},
   "source": [
    "## DRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e8ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import json, hashlib, zipfile, shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ramanspy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "NAME = \"DRS_TissueClassification\"\n",
    "POS_IDS = {\"cortBone\"}  # Positive class\n",
    "NEG_IDS = {\"muscle\"}  # Negative class\n",
    "TEST_SIZE = 0.20\n",
    "RANDOM_SEED = 4\n",
    "\n",
    "# Data paths\n",
    "CSV_PATH = Path.home() / \"local-datasets\" / \"DRS.csv\"\n",
    "OUT_CSV = Path.home() / \"local-datasets\" / \"drs_tissue.csv\"\n",
    "\n",
    "# wildboar repository settings\n",
    "REPO_ROOT = Path.home() / \"local-datasets\"\n",
    "HTTP_BASE = \"http://127.0.0.1:8765\"\n",
    "MANIFEST_NAME = \"repo2.json\"\n",
    "REPO_NAME = \"repotwo\"\n",
    "BUNDLE_VERSION = \"1.2\"\n",
    "BUNDLE_TAG = \"default\"\n",
    "\n",
    "ZIP_NAME_NO_EXT = f\"ucr-v{BUNDLE_VERSION}-{BUNDLE_TAG}\"\n",
    "ZIP_NAME = f\"{ZIP_NAME_NO_EXT}.zip\"\n",
    "SHA_NAME = f\"{ZIP_NAME_NO_EXT}.sha1\"\n",
    "\n",
    "\n",
    "def _sha1(path: Path) -> str:\n",
    "    return hashlib.sha1(path.read_bytes()).hexdigest()\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Loading DRS data...\")\n",
    "    \n",
    "    # Load DRS CSV\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    \n",
    "    # Remove index column if present\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop(columns=['Unnamed: 0'])\n",
    "    \n",
    "    # Extract features (wavelength columns) and labels\n",
    "    feature_cols = [col for col in df.columns if col != 'target_y']\n",
    "    wavelengths = np.array([float(col) for col in feature_cols])\n",
    "    \n",
    "    X_all_raw = df[feature_cols].to_numpy(np.float32)\n",
    "    y_all = df['target_y'].values\n",
    "    \n",
    "    print(f\"Loaded DRS data: {X_all_raw.shape[0]} samples, {X_all_raw.shape[1]} wavelengths\")\n",
    "    print(f\"Wavelength range: {wavelengths.min():.1f} - {wavelengths.max():.1f} nm\")\n",
    "    print(f\"Class distribution: {dict(zip(*np.unique(y_all, return_counts=True)))}\")\n",
    "    \n",
    "    print(\"Applying DRS-appropriate preprocessing...\")\n",
    "    \n",
    "    # Apply basic normalisation\n",
    "    X_all_preprocessed = (X_all_raw - X_all_raw.min(axis=1, keepdims=True)) / (\n",
    "        X_all_raw.max(axis=1, keepdims=True) - X_all_raw.min(axis=1, keepdims=True)\n",
    "    )\n",
    "    \n",
    "    # Convert to the expected format (mimic ramanspy output structure)\n",
    "    class DRSSpectra:\n",
    "        def __init__(self, data):\n",
    "            self.spectral_data = data.astype(np.float32)\n",
    "    \n",
    "    X_all_preprocessed = DRSSpectra(X_all_preprocessed)\n",
    "\n",
    "    print(\"Subsetting and splitting preprocessed data...\")\n",
    "    mask = np.isin(y_all, list(POS_IDS | NEG_IDS))\n",
    "    X = X_all_preprocessed.spectral_data[mask].astype(np.float32)\n",
    "    y = y_all[mask]\n",
    "    y_bin = np.where(np.isin(y, list(POS_IDS)), 1, 0).astype(np.int64)\n",
    "\n",
    "    print(f\"Binary classification setup:\")\n",
    "    print(f\"  Positive classes: {POS_IDS}\")\n",
    "    print(f\"  Negative classes: {NEG_IDS}\")\n",
    "    print(f\"  Binary samples: {X.shape[0]} (Positive: {(y_bin==1).sum()}, Negative: {(y_bin==0).sum()})\")\n",
    "\n",
    "    # Save full preprocessed CSV\n",
    "    headers = [f\"{wl:.3f}\" for wl in wavelengths]\n",
    "    df_preprocessed = pd.DataFrame(X_all_preprocessed.spectral_data, columns=headers)\n",
    "    df_preprocessed[\"tissue_type\"] = y_all\n",
    "    df_preprocessed.to_csv(OUT_CSV, index=False)\n",
    "    print(\"Wrote preprocessed CSV ->\", OUT_CSV)\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y_bin, test_size=TEST_SIZE, stratify=y_bin, random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    tmp = REPO_ROOT / \"tmp_ucr_build\"\n",
    "    (tmp / \"ucr\").mkdir(parents=True, exist_ok=True)\n",
    "    np.savez_compressed(tmp / \"ucr\" / f\"{NAME}_TRAIN.npz\", x=X_tr, y=y_tr)\n",
    "    np.savez_compressed(tmp / \"ucr\" / f\"{NAME}_TEST.npz\",  x=X_te, y=y_te)\n",
    "\n",
    "    zip_path = REPO_ROOT / ZIP_NAME\n",
    "    if zip_path.exists():\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            z.extractall(tmp)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
    "        for p in sorted((tmp / \"ucr\").glob(\"*\")):\n",
    "            if p.is_file():\n",
    "                z.write(p, arcname=f\"ucr/{p.name}\")\n",
    "    \n",
    "    (REPO_ROOT / SHA_NAME).write_text(_sha1(zip_path))\n",
    "\n",
    "    manifest_path = REPO_ROOT / MANIFEST_NAME\n",
    "    if manifest_path.exists():\n",
    "        manifest = json.loads(manifest_path.read_text())\n",
    "    else:\n",
    "        manifest = {\n",
    "            \"name\": REPO_NAME,\n",
    "            \"version\": \"1.0\",\n",
    "            \"wildboar_requires\": \"1.0\",\n",
    "            \"bundle_url\": f\"{HTTP_BASE}/{{bundle}}-v{{version}}-{{tag}}\",\n",
    "            \"bundles\": []\n",
    "        }\n",
    "    \n",
    "    manifest[\"bundle_url\"] = f\"{HTTP_BASE}/{{bundle}}-v{{version}}-{{tag}}\"\n",
    "    bundles = manifest.setdefault(\"bundles\", [])\n",
    "    ucr = next((b for b in bundles if b.get(\"key\") == \"ucr\" and b.get(\"version\") == BUNDLE_VERSION and b.get(\"tag\", \"default\") == BUNDLE_TAG), None)\n",
    "    \n",
    "    if ucr is None:\n",
    "        ucr = {\"key\": \"ucr\", \"name\": \"ucr\", \"version\": BUNDLE_VERSION, \"tag\": BUNDLE_TAG, \"datasets\": []}\n",
    "        bundles.append(ucr)\n",
    "\n",
    "    def ensure(part: str):\n",
    "        fname = f\"{NAME}_{'TRAIN' if part == 'train' else 'TEST'}.npz\"\n",
    "        if not any(d.get(\"name\") == NAME and d.get(\"part\") == part for d in ucr[\"datasets\"]):\n",
    "            ucr[\"datasets\"].append({\"name\": NAME, \"file\": fname, \"part\": part})\n",
    "            \n",
    "    ensure(\"train\"); ensure(\"test\")\n",
    "\n",
    "    manifest_path.write_text(json.dumps(manifest, indent=2))\n",
    "    shutil.rmtree(tmp, ignore_errors=True)\n",
    "    \n",
    "    print(f\"\\nTrain set: {X_tr.shape} (Positive: {(y_tr==1).sum()}, Negative: {(y_tr==0).sum()})\")\n",
    "    print(f\"Test set:  {X_te.shape} (Positive: {(y_te==1).sum()}, Negative: {(y_te==0).sum()})\")\n",
    "    print(\"\\n Built and added new preprocessed DRS dataset to the repository.\")\n",
    "    print(f\"   Dataset name: '{NAME}'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a16688",
   "metadata": {},
   "source": [
    "# Bacteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f3e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import json, hashlib, zipfile, shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import ramanspy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "NAME = \"EcoliVsKpneumoniae_ramanspy_singular\"\n",
    "POS_IDS = {3} #E. coli\n",
    "NEG_IDS = {9} #K. pneumoniae\n",
    "TEST_SIZE = 0.20\n",
    "RANDOM_SEED = 4\n",
    "\n",
    "RAMAN_DIR = Path.home() / \"data\" / \"raman\"\n",
    "X_REF = RAMAN_DIR / \"X_reference.npy\"\n",
    "Y_REF = RAMAN_DIR / \"y_reference.npy\"\n",
    "\n",
    "# wildboar repository settings\n",
    "REPO_ROOT = Path.home() / \"local-datasets\"\n",
    "HTTP_BASE = \"http://127.0.0.1:8765\"\n",
    "MANIFEST_NAME = \"repo2.json\"\n",
    "REPO_NAME = \"repotwo\"\n",
    "BUNDLE_VERSION = \"1.2\"\n",
    "BUNDLE_TAG = \"default\"\n",
    "\n",
    "ZIP_NAME_NO_EXT = f\"ucr-v{BUNDLE_VERSION}-{BUNDLE_TAG}\"\n",
    "ZIP_NAME = f\"{ZIP_NAME_NO_EXT}.zip\"\n",
    "SHA_NAME = f\"{ZIP_NAME_NO_EXT}.sha1\"\n",
    "\n",
    "\n",
    "def _sha1(path: Path) -> str:\n",
    "    return hashlib.sha1(path.read_bytes()).hexdigest()\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Loading raw data...\")\n",
    "    X_all_raw = np.load(X_REF)\n",
    "    y_all = np.load(Y_REF)\n",
    "\n",
    "    raman_spectra = ramanspy.Spectrum(X_all_raw, np.arange(X_all_raw.shape[1]))\n",
    "\n",
    "    print(\"Applying ramanspy preprocessing...\")\n",
    "    \n",
    "    pipeline = ramanspy.preprocessing.Pipeline([\n",
    "        ramanspy.preprocessing.despike.WhitakerHayes(),\n",
    "        ramanspy.preprocessing.baseline.ASLS(),\n",
    "        ramanspy.preprocessing.normalise.MinMax(),\n",
    "    ])\n",
    "    \n",
    "    X_all_preprocessed = pipeline.apply(raman_spectra)\n",
    "\n",
    "    print(\"Subsetting and splitting preprocessed data...\")\n",
    "    mask = np.isin(y_all, list(POS_IDS | NEG_IDS))\n",
    "    X = X_all_preprocessed.spectral_data[mask].astype(np.float32)\n",
    "    y = y_all[mask]\n",
    "    y_bin = np.where(np.isin(y, list(POS_IDS)), 1, 0).astype(np.int64)\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y_bin, test_size=TEST_SIZE, stratify=y_bin, random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    tmp = REPO_ROOT / \"tmp_ucr_build\"\n",
    "    (tmp / \"ucr\").mkdir(parents=True, exist_ok=True)\n",
    "    np.savez_compressed(tmp / \"ucr\" / f\"{NAME}_TRAIN.npz\", x=X_tr, y=y_tr)\n",
    "    np.savez_compressed(tmp / \"ucr\" / f\"{NAME}_TEST.npz\",  x=X_te, y=y_te)\n",
    "\n",
    "    zip_path = REPO_ROOT / ZIP_NAME\n",
    "    if zip_path.exists():\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            z.extractall(tmp)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
    "        for p in sorted((tmp / \"ucr\").glob(\"*\")):\n",
    "            if p.is_file():\n",
    "                z.write(p, arcname=f\"ucr/{p.name}\")\n",
    "    \n",
    "    (REPO_ROOT / SHA_NAME).write_text(_sha1(zip_path))\n",
    "\n",
    "    manifest_path = REPO_ROOT / MANIFEST_NAME\n",
    "    if manifest_path.exists():\n",
    "        manifest = json.loads(manifest_path.read_text())\n",
    "    else:\n",
    "        manifest = {\n",
    "            \"name\": REPO_NAME,\n",
    "            \"version\": \"1.0\",\n",
    "            \"wildboar_requires\": \"1.0\",\n",
    "            \"bundle_url\": f\"{HTTP_BASE}/{{bundle}}-v{{version}}-{{tag}}\",\n",
    "            \"bundles\": []\n",
    "        }\n",
    "    \n",
    "    manifest[\"bundle_url\"] = f\"{HTTP_BASE}/{{bundle}}-v{{version}}-{{tag}}\"\n",
    "    bundles = manifest.setdefault(\"bundles\", [])\n",
    "    ucr = next((b for b in bundles if b.get(\"key\") == \"ucr\" and b.get(\"version\") == BUNDLE_VERSION and b.get(\"tag\", \"default\") == BUNDLE_TAG), None)\n",
    "    \n",
    "    if ucr is None:\n",
    "        ucr = {\"key\": \"ucr\", \"name\": \"ucr\", \"version\": BUNDLE_VERSION, \"tag\": BUNDLE_TAG, \"datasets\": []}\n",
    "        bundles.append(ucr)\n",
    "\n",
    "    def ensure(part: str):\n",
    "        fname = f\"{NAME}_{'TRAIN' if part == 'train' else 'TEST'}.npz\"\n",
    "        if not any(d.get(\"name\") == NAME and d.get(\"part\") == part for d in ucr[\"datasets\"]):\n",
    "            ucr[\"datasets\"].append({\"name\": NAME, \"file\": fname, \"part\": part})\n",
    "    ensure(\"train\"); ensure(\"test\")\n",
    "\n",
    "    manifest_path.write_text(json.dumps(manifest, indent=2))\n",
    "    shutil.rmtree(tmp, ignore_errors=True)\n",
    "    \n",
    "    print(\"\\n Built and added new preprocessed dataset to the repository.\")\n",
    "    print(f\"  Dataset name: '{NAME}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df1ee30",
   "metadata": {},
   "source": [
    "To load these datasets fo glacier and rsf use:\n",
    "\n",
    "cd /home/cok7/local-datasets && python -m http.server 8765\n",
    "\n",
    "Then in Python:\n",
    "  from wildboar.datasets import install_repository, refresh_repositories, list_repositories, list_datasets\n",
    "  install_repository('http://127.0.0.1:8765/repo2.json', refresh=True)\n",
    "  print(list_repositories())\n",
    "  print(list_datasets('repotwo/ucr'))  # should now include added datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96035945",
   "metadata": {},
   "source": [
    "# Without Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wildboar.datasets import install_repository, refresh_repositories, list_repositories, list_datasets\n",
    "#install_repository('http://127.0.0.1:8765/repo2.json', refresh=True)\n",
    "print(list_repositories())\n",
    "print(list_datasets('repotwo/ucr', force=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac4b126",
   "metadata": {},
   "source": [
    "# CELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2412c6",
   "metadata": {},
   "source": [
    "# Bacteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abae34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import ramanspy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "NAME = \"EcoliVsKpneumoniae_ramanspy_singular\"\n",
    "\n",
    "POS_IDS = {3}\n",
    "NEG_IDS = {9}\n",
    "TEST_SIZE = 0.20\n",
    "RANDOM_SEED = 4\n",
    "\n",
    "RAMAN_DIR = Path.home() / \"data\" / \"raman\"\n",
    "X_REF = RAMAN_DIR / \"X_reference.npy\"\n",
    "Y_REF = RAMAN_DIR / \"y_reference.npy\"\n",
    "\n",
    "OUT_DIR = Path(\"cels_datasets\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def main():\n",
    "    print(\"Loading raw data...\")\n",
    "    X_all_raw = np.load(X_REF)\n",
    "    y_all = np.load(Y_REF)\n",
    "    \n",
    "    raman_spectra = ramanspy.Spectrum(X_all_raw, np.arange(X_all_raw.shape[1]))\n",
    "\n",
    "    print(\"Applying ramanspy preprocessing...\")\n",
    "    pipeline = ramanspy.preprocessing.Pipeline([\n",
    "        ramanspy.preprocessing.despike.WhitakerHayes(),\n",
    "        ramanspy.preprocessing.baseline.ASLS(),\n",
    "        ramanspy.preprocessing.normalise.MinMax(),\n",
    "    ])\n",
    "    X_all_preprocessed = pipeline.apply(raman_spectra)\n",
    "\n",
    "    print(\"Subsetting and splitting preprocessed data...\")\n",
    "    mask = np.isin(y_all, list(POS_IDS | NEG_IDS))\n",
    "    \n",
    "    X = X_all_preprocessed.spectral_data[mask].astype(np.float32)\n",
    "    y = y_all[mask]\n",
    "    y_bin = np.where(np.isin(y, list(POS_IDS)), 1, 0).astype(np.int64)\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y_bin, test_size=TEST_SIZE, stratify=y_bin, random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    output_path = OUT_DIR / f\"{NAME}.npz\"\n",
    "    print(f\"Saving new preprocessed dataset to {output_path}...\")\n",
    "    np.savez_compressed(\n",
    "        output_path,\n",
    "        X_train=X_tr, y_train=y_tr,\n",
    "        X_test=X_te,  y_test=y_te\n",
    "    )\n",
    "    \n",
    "    label_map = {\n",
    "        \"binary\": {\"1\": sorted(list(POS_IDS)), \"0\": sorted(list(NEG_IDS))},\n",
    "        \"note\": \"binary labels correspond to these original class IDs\"\n",
    "    }\n",
    "    (OUT_DIR / f\"{NAME}_labelmap.json\").write_text(json.dumps(label_map, indent=2))\n",
    "\n",
    "    print(f\"Wrote preprocessed data to {output_path.resolve()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a623ff",
   "metadata": {},
   "source": [
    "# COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f44929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ramanspy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.io import loadmat\n",
    "\n",
    "NAME = \"RamanCOVID19_ramanspy_preprocessed\"\n",
    "\n",
    "POS_IDS = {\"COVID-19\"}\n",
    "NEG_IDS = {\"Healthy\"}\n",
    "TEST_SIZE = 0.20\n",
    "RANDOM_SEED = 4\n",
    "\n",
    "ROOT = Path.home() / \"local-datasets\" / \"covid19\"\n",
    "CLASS_FILES = {\n",
    "    \"COVID-19\":   \"raw_COVID.txt\",\n",
    "    \"Healthy\":    \"raw_Helthy.txt\",\n",
    "    \"Suspected\":  \"raw_Suspected.txt\",\n",
    "}\n",
    "\n",
    "OUT_DIR = Path(\"/home/cok7/MScProject/cels_datasets\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def read_matrix(p: Path) -> np.ndarray:\n",
    "    return pd.read_csv(p, header=None, sep=r\"[\\s,;]+\", engine=\"python\").values\n",
    "\n",
    "def get_wavenumbers_from_mat(mat_path: Path) -> np.ndarray:\n",
    "    md = loadmat(mat_path)\n",
    "    cands = []\n",
    "    for k, v in md.items():\n",
    "        if not isinstance(v, np.ndarray):\n",
    "            continue\n",
    "        arr = v.squeeze()\n",
    "        if arr.ndim == 1 and 200 < arr.size < 5000:\n",
    "            if np.all((arr >= 350) & (arr <= 4000)):\n",
    "                cands.append(arr.astype(float))\n",
    "    return max(cands, key=lambda a: a.size)\n",
    "\n",
    "def main():\n",
    "    print(\"Loading raw data...\")\n",
    "    \n",
    "    # Get wavenumbers from MAT file\n",
    "    wn = get_wavenumbers_from_mat(ROOT / \"data.mat\")\n",
    "    B = wn.size\n",
    "\n",
    "    # Read class matrices\n",
    "    X_list, y_list = [], []\n",
    "    for label, fname in CLASS_FILES.items():\n",
    "        mat = read_matrix(ROOT / fname)\n",
    "        if mat.shape[1] != B and mat.shape[0] == B:\n",
    "            mat = mat.T\n",
    "        elif mat.shape[1] != B and mat.shape[0] != B:\n",
    "            raise ValueError(f\"{fname}: shape {mat.shape} doesn't match wavenumber length {B}\")\n",
    "        Xc = mat.astype(np.float32)\n",
    "        X_list.append(Xc)\n",
    "        y_list.append(np.full(Xc.shape[0], label, dtype=object))\n",
    "\n",
    "    X_all_raw = np.vstack(X_list)\n",
    "    y_all = np.concatenate(y_list)\n",
    "    \n",
    "    raman_spectra = ramanspy.Spectrum(X_all_raw, wn)\n",
    "\n",
    "    print(\"Applying ramanspy preprocessing...\")\n",
    "    pipeline = ramanspy.preprocessing.Pipeline([\n",
    "        ramanspy.preprocessing.despike.WhitakerHayes(),\n",
    "        ramanspy.preprocessing.baseline.ASLS(),\n",
    "        ramanspy.preprocessing.normalise.MinMax(),\n",
    "    ])\n",
    "    X_all_preprocessed = pipeline.apply(raman_spectra)\n",
    "\n",
    "    print(\"Subsetting and splitting preprocessed data...\")\n",
    "    mask = np.isin(y_all, list(POS_IDS | NEG_IDS))\n",
    "    \n",
    "    X = X_all_preprocessed.spectral_data[mask].astype(np.float32)\n",
    "    y = y_all[mask]\n",
    "    y_bin = np.where(np.isin(y, list(POS_IDS)), 1, 0).astype(np.int64)\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y_bin, test_size=TEST_SIZE, stratify=y_bin, random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    output_path = OUT_DIR / f\"{NAME}.npz\"\n",
    "    print(f\"Saving new preprocessed dataset to {output_path}...\")\n",
    "    np.savez_compressed(\n",
    "        output_path,\n",
    "        X_train=X_tr, y_train=y_tr,\n",
    "        X_test=X_te,  y_test=y_te\n",
    "    )\n",
    "    \n",
    "    label_map = {\n",
    "        \"binary\": {\"1\": sorted(list(POS_IDS)), \"0\": sorted(list(NEG_IDS))},\n",
    "        \"note\": \"binary labels correspond to these original class IDs\"\n",
    "    }\n",
    "    (OUT_DIR / f\"{NAME}_labelmap.json\").write_text(json.dumps(label_map, indent=2))\n",
    "\n",
    "    print(f\"Wrote preprocessed data to {output_path.resolve()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410b3f66",
   "metadata": {},
   "source": [
    "# DRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ecac8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DRS data...\n",
      "Loaded DRS data: 5215 samples, 1531 wavelengths\n",
      "Wavelength range: 355.0 - 1849.7 nm\n",
      "Class distribution: {'boneCement': 215, 'boneMarrow': 1000, 'cartilage': 1000, 'cortBone': 1000, 'muscle': 1000, 'traBone': 1000}\n",
      "Applying DRS-appropriate preprocessing...\n",
      "Subsetting and splitting preprocessed data...\n",
      "Binary classification setup:\n",
      "  Positive classes: {'cortBone'}\n",
      "  Negative classes: {'muscle'}\n",
      "  Binary samples: 2000 (Positive: 1000, Negative: 1000)\n",
      "Saving new preprocessed dataset to /home/cok7/MScProject/cels_datasets/DRS_TissueClassification.npz...\n",
      "Train set: (1600, 1531) (Positive: 800, Negative: 800)\n",
      "Test set:  (400, 1531) (Positive: 200, Negative: 200)\n",
      "Wrote preprocessed data to /home/cok7/MScProject/cels_datasets/DRS_TissueClassification.npz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "NAME = \"DRS_TissueClassification\"\n",
    "\n",
    "POS_IDS = {\"cortBone\"}\n",
    "NEG_IDS = {\"muscle\"}\n",
    "TEST_SIZE = 0.20\n",
    "RANDOM_SEED = 4\n",
    "\n",
    "CSV_PATH = Path.home() / \"local-datasets\" / \"DRS.csv\"\n",
    "\n",
    "OUT_DIR = Path(\"/home/cok7/MScProject/cels_datasets\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def main():\n",
    "    print(\"Loading DRS data...\")\n",
    "    \n",
    "    # Load DRS CSV\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    \n",
    "    # Remove index column\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop(columns=['Unnamed: 0'])\n",
    "    \n",
    "    # Extract features and labels\n",
    "    feature_cols = [col for col in df.columns if col != 'target_y']\n",
    "    wavelengths = np.array([float(col) for col in feature_cols])\n",
    "    \n",
    "    X_all_raw = df[feature_cols].to_numpy(np.float32)\n",
    "    y_all = df['target_y'].values\n",
    "    \n",
    "    print(f\"Loaded DRS data: {X_all_raw.shape[0]} samples, {X_all_raw.shape[1]} wavelengths\")\n",
    "    print(f\"Wavelength range: {wavelengths.min():.1f} - {wavelengths.max():.1f} nm\")\n",
    "    print(f\"Class distribution: {dict(zip(*np.unique(y_all, return_counts=True)))}\")\n",
    "    \n",
    "    print(\"Applying DRS-appropriate preprocessing...\")\n",
    "    \n",
    "    # Apply normalisation\n",
    "    X_all_preprocessed = (X_all_raw - X_all_raw.min(axis=1, keepdims=True)) / (\n",
    "        X_all_raw.max(axis=1, keepdims=True) - X_all_raw.min(axis=1, keepdims=True)\n",
    "    )\n",
    "\n",
    "    print(\"Subsetting and splitting preprocessed data...\")\n",
    "    mask = np.isin(y_all, list(POS_IDS | NEG_IDS))\n",
    "    \n",
    "    X = X_all_preprocessed[mask].astype(np.float32)\n",
    "    y = y_all[mask]\n",
    "    y_bin = np.where(np.isin(y, list(POS_IDS)), 1, 0).astype(np.int64)\n",
    "\n",
    "    print(f\"Binary classification setup:\")\n",
    "    print(f\"  Positive classes: {POS_IDS}\")\n",
    "    print(f\"  Negative classes: {NEG_IDS}\")\n",
    "    print(f\"  Binary samples: {X.shape[0]} (Positive: {(y_bin==1).sum()}, Negative: {(y_bin==0).sum()})\")\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y_bin, test_size=TEST_SIZE, stratify=y_bin, random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    output_path = OUT_DIR / f\"{NAME}.npz\"\n",
    "    print(f\"Saving new preprocessed dataset to {output_path}...\")\n",
    "    np.savez_compressed(\n",
    "        output_path,\n",
    "        X_train=X_tr, y_train=y_tr,\n",
    "        X_test=X_te,  y_test=y_te\n",
    "    )\n",
    "    \n",
    "    label_map = {\n",
    "        \"binary\": {\"1\": sorted(list(POS_IDS)), \"0\": sorted(list(NEG_IDS))},\n",
    "        \"note\": \"DRS tissue classification - binary labels correspond to these tissue types\",\n",
    "        \"wavelength_range\": f\"{wavelengths.min():.1f}-{wavelengths.max():.1f} nm\",\n",
    "        \"preprocessing\": \"Min-max normalisation\",\n",
    "        \"total_samples\": int(X.shape[0]),\n",
    "        \"train_samples\": int(len(X_tr)),\n",
    "        \"test_samples\": int(len(X_te))\n",
    "    }\n",
    "    (OUT_DIR / f\"{NAME}_labelmap.json\").write_text(json.dumps(label_map, indent=2))\n",
    "\n",
    "    print(f\"Train set: {X_tr.shape} (Positive: {(y_tr==1).sum()}, Negative: {(y_tr==0).sum()})\")\n",
    "    print(f\"Test set:  {X_te.shape} (Positive: {(y_te==1).sum()}, Negative: {(y_te==0).sum()})\")\n",
    "    print(f\"Wrote preprocessed data to {output_path.resolve()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
